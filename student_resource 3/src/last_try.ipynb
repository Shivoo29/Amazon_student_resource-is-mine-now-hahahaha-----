{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/codemaster29/miniconda3/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: einops in /home/codemaster29/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: filelock in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codemaster29/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor , Blip2ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codemaster29/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5acc4ef5964dd7a056161e8c09e9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m Blip2ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip2-opt-2.7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     30\u001b[0m                                                       load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     31\u001b[0m                                                       device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Auto maps model layers across devices if necessary\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Send the model to the device (GPU or CPU)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/big_modeling.py:457\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:2883\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES:\n\u001b[0;32m-> 2883\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2884\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2885\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2886\u001b[0m     )\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ:\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m     dtype_present_in_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "'''# pip install accelerate bitsandbytes\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "question = \"how many dogs are in the picture?\"\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True).strip())\n",
    "'''\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "# Check if a GPU (CUDA) is available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor and model with 8-bit precision to reduce memory usage\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", \n",
    "                                                      load_in_8bit=True, \n",
    "                                                      device_map=\"auto\")  # Auto maps model layers across devices if necessary\n",
    "\n",
    "# Send the model to the device (GPU or CPU)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39d9d684b4a42cb872f24c561230526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1da501f84b0435f8478b418a1eb6088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_moondream.py:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681c2d52972b456282ac02a17021815f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "moondream.py:   0%|          | 0.00/7.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7747e2007a0248d2a922cc8e93b313dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "region_model.py:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3d775e3f2947449574f9d4633ffd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fourier_features.py:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1932847dc84acd8170ebd80b54e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vision_encoder.py:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64effaf4d414277857a684d3f21f0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi.py:   0%|          | 0.00/63.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97df2fb8c7214c5182d8d99020cd1933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93284dd48484ecb98c1df10833f48ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d8a8e2cc014e348e9c77d45f2e899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76530fb7524a7792963f826c945ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f233bf2117794e6281e0f50f349cd7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a283d2f8814b72a49de98604815e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b7dca636c24891bd6642869d63b1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a56795c4d9048d08ecdba1b9b3ee2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Moondream(\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (encoder): EncoderWrapper(\n",
       "      (model): ModuleDict(\n",
       "        (visual): VisionTransformer(\n",
       "          (patch_embed): LinearPatchEmbedding(\n",
       "            (linear): Linear(in_features=588, out_features=1152, bias=True)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (24): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (25): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (26): VitBlock(\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (act): GELU(approximate='tanh')\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection): VisionProjection(\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2304, out_features=8192, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (region_model): RegionModel(\n",
       "    (position_features): FourierFeatures()\n",
       "    (position_encoder): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (size_features): FourierFeatures()\n",
       "    (size_encoder): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (position_decoder): Linear(in_features=2048, out_features=2, bias=True)\n",
       "    (size_decoder): Linear(in_features=2048, out_features=2, bias=True)\n",
       "    (confidence_decoder): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       "  (text_model): PhiForCausalLM(\n",
       "    (transformer): PhiModel(\n",
       "      (embd): Embedding(\n",
       "        (wte): Embedding(51200, 2048)\n",
       "      )\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x PhiDecoderLayer(\n",
       "          (mixer): PhiSdpaAttention(\n",
       "            (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (rotary_emb): PhiRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): PhiMLP(\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): CausalLMHead(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "#processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "#model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")model.to(device)\n",
    "#model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-08-26\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the processor and model with 8-bit precision to reduce memory usage\u001b[39;00m\n\u001b[1;32m     10\u001b[0m processor \u001b[38;5;241m=\u001b[39m Blip2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip2-opt-2.7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBlip2ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip2-opt-2.7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Auto maps model layers across devices if necessary\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example image and question\u001b[39;00m\n\u001b[1;32m     18\u001b[0m img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://m.media-amazon.com/images/I/41-NCxNuBxL.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3909\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3906\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3909\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3912\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:87\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     84\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     85\u001b[0m     }\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.37.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "# Check if a GPU (CUDA) is available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor and model with 8-bit precision to reduce memory usage\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"  # Auto maps model layers across devices if necessary\n",
    ")\n",
    "\n",
    "# Example image and question\n",
    "img_url = 'https://m.media-amazon.com/images/I/41-NCxNuBxL.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "question = \"what is the width of the object shown in the image? \"\n",
    "\n",
    "# Preprocess the image and question\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# Generate output\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the result\n",
    "print(processor.decode(out[0], skip_special_tokens=True).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Blip2ForConditionalGeneration.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m processor \u001b[38;5;241m=\u001b[39m Blip2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip2-opt-2.7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the model with 8-bit quantization, enabling fp32 CPU offload and custom device map\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBlip2ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip2-opt-2.7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Auto map layers across devices\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit_fp32_cpu_offload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Offload certain modules to CPU in 32-bit precision\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Example image and question\u001b[39;00m\n\u001b[1;32m     24\u001b[0m img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3832\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3827\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3828\u001b[0m )\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3835\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: Blip2ForConditionalGeneration.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# Check if a GPU (CUDA) is available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the configuration for 8-bit quantization using BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load the processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Load the model with 8-bit quantization, enabling fp32 CPU offload and custom device map\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Auto map layers across devices\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Offload certain modules to CPU in 32-bit precision\n",
    ")\n",
    "\n",
    "# Example image and question\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "question = \"How many dogs are in the picture?\"\n",
    "\n",
    "# Preprocess the image and question\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# Generate output\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the result\n",
    "print(processor.decode(out[0], skip_special_tokens=True).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 cm\n"
     ]
    }
   ],
   "source": [
    "url = \"https://m.media-amazon.com/images/I/41-NCxNuBxL.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image\n",
    "#image = Image.open('<IMAGE_PATH>')\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"tell me the width of the object\", tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
